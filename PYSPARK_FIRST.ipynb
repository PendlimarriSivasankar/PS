{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV21hOJqvxVaonywjiJkN2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PendlimarriSivasankar/PS/blob/main/PYSPARK_FIRST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shrhIjX1PMlW",
        "outputId": "62a042b4-0d82-4ddd-ddef-1c5534ee585b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE==== START YOUR WORKðŸ‘‡ \n"
          ]
        }
      ],
      "source": [
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/df.csv -O df.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/df1.csv -O df1.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/dt.txt -O dt.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file1.txt -O file1.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file2.txt -O file2.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file3.txt -O file3.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file4.json -O file4.json\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file5.parquet -O file5.parquet\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file6 -O file6\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/prod.csv -O prod.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/state.txt -O state.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/usdata.csv -O usdata.csv\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import sys\n",
        "import os\n",
        "python_path = sys.executable\n",
        "os.environ['PYSPARK_PYTHON'] = python_path\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "conf = SparkConf().setAppName(\"pyspark\").setMaster(\"local[*]\").set(\"spark.driver.host\",\"localhost\").set(\"spark.driver.allowMultipleContexts\",\"true\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.config(\"spark.driver.allowMultipleContexts\",\"true\").getOrCreate()\n",
        "print(\"DONE==== START YOUR WORKðŸ‘‡ \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"FilterProducts\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (0, 'Y', 'N'),\n",
        "    (1, 'Y', 'Y'),\n",
        "    (2, 'N', 'Y'),\n",
        "    (3, 'Y', 'Y'),\n",
        "    (4, 'N', 'N')\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"product_id\", \"low_fats\", \"recyclable\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Filter products that are low in fats and recyclable\n",
        "filtered_df = df.filter((col(\"low_fats\") == 'Y') & (col(\"recyclable\") == 'Y'))\n",
        "\n",
        "# Show result\n",
        "filtered_df.select(\"product_id\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsNLKbhpP0yl",
        "outputId": "9ce2b386-4b57-44f2-c86a-4535947965dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+\n",
            "|product_id|low_fats|recyclable|\n",
            "+----------+--------+----------+\n",
            "|         0|       Y|         N|\n",
            "|         1|       Y|         Y|\n",
            "|         2|       N|         Y|\n",
            "|         3|       Y|         Y|\n",
            "|         4|       N|         N|\n",
            "+----------+--------+----------+\n",
            "\n",
            "+----------+\n",
            "|product_id|\n",
            "+----------+\n",
            "|         1|\n",
            "|         3|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"FilterCustomers\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Will\", None),\n",
        "    (2, \"Jane\", None),\n",
        "    (3, \"Alex\", 2),\n",
        "    (4, \"Bill\", None),\n",
        "    (5, \"Zack\", 1),\n",
        "    (6, \"Mark\", 2)\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"id\", \"name\", \"referee_id\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "# Filter customers where referee_id is null or not equal to 2\n",
        "filtered_df = df.filter((col(\"referee_id\").isNull()) | (col(\"referee_id\") != 2))\n",
        "\n",
        "# Show result\n",
        "filtered_df.select(\"name\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceg6ZDM7T9-x",
        "outputId": "4d537c60-cf77-4328-814a-225f66322225"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----------+\n",
            "| id|name|referee_id|\n",
            "+---+----+----------+\n",
            "|  1|Will|      NULL|\n",
            "|  2|Jane|      NULL|\n",
            "|  3|Alex|         2|\n",
            "|  4|Bill|      NULL|\n",
            "|  5|Zack|         1|\n",
            "|  6|Mark|         2|\n",
            "+---+----+----------+\n",
            "\n",
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|Will|\n",
            "|Jane|\n",
            "|Bill|\n",
            "|Zack|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BigCountries\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000000),\n",
        "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000000),\n",
        "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000),\n",
        "    (\"Andorra\", \"Europe\", 468, 78115, 3712000000),\n",
        "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000000)\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"name\", \"continent\", \"area\", \"population\", \"gdp\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "# Filter big countries\n",
        "big_countries_df = df.filter((col(\"area\") >= 3000000) | (col(\"population\") >= 25000000))\n",
        "\n",
        "# Select required columns\n",
        "result_df = big_countries_df.select(\"name\", \"population\", \"area\")\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGzsuptRUHCW",
        "outputId": "917409b6-a341-43e8-eec2-2b79d4b27903"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------+----------+------------+\n",
            "|       name|continent|   area|population|         gdp|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n",
            "|    Albania|   Europe|  28748|   2831741| 12960000000|\n",
            "|    Algeria|   Africa|2381741|  37100000|188681000000|\n",
            "|    Andorra|   Europe|    468|     78115|  3712000000|\n",
            "|     Angola|   Africa|1246700|  20609294|100990000000|\n",
            "+-----------+---------+-------+----------+------------+\n",
            "\n",
            "+-----------+----------+-------+\n",
            "|       name|population|   area|\n",
            "+-----------+----------+-------+\n",
            "|Afghanistan|  25500100| 652230|\n",
            "|    Algeria|  37100000|2381741|\n",
            "+-----------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"ActiveViewers\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, 3, 5, \"2019-08-01\"),\n",
        "    (1, 3, 6, \"2019-08-02\"),\n",
        "    (2, 7, 7, \"2019-08-01\"),\n",
        "    (2, 7, 6, \"2019-08-02\"),\n",
        "    (4, 7, 1, \"2019-07-22\"),\n",
        "    (3, 4, 4, \"2019-07-21\"),\n",
        "    (3, 4, 4, \"2019-07-21\")\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"article_id\", \"author_id\", \"viewer_id\", \"view_date\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "# Filter where viewer_id is not the same as author_id\n",
        "filtered_df = df.filter(col(\"viewer_id\") != col(\"author_id\"))\n",
        "\n",
        "# Select distinct viewer IDs\n",
        "result_df = filtered_df.select(col(\"viewer_id\").alias(\"id\")).distinct()\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c34N8LiIU4OV",
        "outputId": "11dd9279-a2e4-41ee-f126-75cb38d5f3cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---------+----------+\n",
            "|article_id|author_id|viewer_id| view_date|\n",
            "+----------+---------+---------+----------+\n",
            "|         1|        3|        5|2019-08-01|\n",
            "|         1|        3|        6|2019-08-02|\n",
            "|         2|        7|        7|2019-08-01|\n",
            "|         2|        7|        6|2019-08-02|\n",
            "|         4|        7|        1|2019-07-22|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "|         3|        4|        4|2019-07-21|\n",
            "+----------+---------+---------+----------+\n",
            "\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  6|\n",
            "|  5|\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"FilterTweets\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Vote for Biden\"),\n",
        "    (2, \"Let us make America great again!\")\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"tweet_id\", \"content\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "# Filter tweets with specific content\n",
        "filtered_df = df.filter(col(\"content\") == \"Let us make America great again!\")\n",
        "\n",
        "# Select tweet_id\n",
        "result_df = filtered_df.select(\"tweet_id\")\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79wxZT7aVQJZ",
        "outputId": "e853cdbe-12e3-4890-c349-c74c57e7ff3f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|tweet_id|             content|\n",
            "+--------+--------------------+\n",
            "|       1|      Vote for Biden|\n",
            "|       2|Let us make Ameri...|\n",
            "+--------+--------------------+\n",
            "\n",
            "+--------+\n",
            "|tweet_id|\n",
            "+--------+\n",
            "|       2|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"EmployeeJoin\").getOrCreate()\n",
        "\n",
        "# Sample data for Employees\n",
        "employees_data = [\n",
        "    (1, \"Alice\"),\n",
        "    (7, \"Bob\"),\n",
        "    (11, \"Meir\"),\n",
        "    (90, \"Winston\"),\n",
        "    (3, \"Jonathan\")\n",
        "]\n",
        "employees_columns = [\"id\", \"name\"]\n",
        "\n",
        "# Sample data for EmployeeUNI\n",
        "employee_uni_data = [\n",
        "    (3, 1),\n",
        "    (11, 2),\n",
        "    (90, 3)\n",
        "]\n",
        "employee_uni_columns = [\"id\", \"unique_id\"]\n",
        "\n",
        "# Create DataFrames\n",
        "employees_df = spark.createDataFrame(employees_data, employees_columns)\n",
        "employees_df.show()\n",
        "employee_uni_df = spark.createDataFrame(employee_uni_data, employee_uni_columns)\n",
        "employee_uni_df.show()\n",
        "# Perform left join\n",
        "joined_df = employees_df.join(employee_uni_df, on=\"id\", how=\"left\")\n",
        "joined_df.show()\n",
        "# Select and reorder columns\n",
        "result_df = joined_df.select(\"unique_id\", \"name\")\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_097s0faJIN",
        "outputId": "542ac77a-9eac-40f5-99a0-3f1428344324"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|    name|\n",
            "+---+--------+\n",
            "|  1|   Alice|\n",
            "|  7|     Bob|\n",
            "| 11|    Meir|\n",
            "| 90| Winston|\n",
            "|  3|Jonathan|\n",
            "+---+--------+\n",
            "\n",
            "+---+---------+\n",
            "| id|unique_id|\n",
            "+---+---------+\n",
            "|  3|        1|\n",
            "| 11|        2|\n",
            "| 90|        3|\n",
            "+---+---------+\n",
            "\n",
            "+---+--------+---------+\n",
            "| id|    name|unique_id|\n",
            "+---+--------+---------+\n",
            "|  7|     Bob|     NULL|\n",
            "|  1|   Alice|     NULL|\n",
            "|  3|Jonathan|        1|\n",
            "| 11|    Meir|        2|\n",
            "| 90| Winston|        3|\n",
            "+---+--------+---------+\n",
            "\n",
            "+---------+--------+\n",
            "|unique_id|    name|\n",
            "+---------+--------+\n",
            "|     NULL|     Bob|\n",
            "|     NULL|   Alice|\n",
            "|        1|Jonathan|\n",
            "|        2|    Meir|\n",
            "|        3| Winston|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"JoinSalesProduct\").getOrCreate()\n",
        "\n",
        "# Sample data for Sales\n",
        "sales_data = [\n",
        "    (1, 100, 2008, 10, 5000),\n",
        "    (2, 100, 2009, 12, 5000),\n",
        "    (7, 200, 2011, 15, 9000)\n",
        "]\n",
        "sales_columns = [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"]\n",
        "\n",
        "# Sample data for Product\n",
        "product_data = [\n",
        "    (100, \"Nokia\"),\n",
        "    (200, \"Apple\"),\n",
        "    (300, \"Samsung\")\n",
        "]\n",
        "product_columns = [\"product_id\", \"product_name\"]\n",
        "\n",
        "# Create DataFrames\n",
        "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
        "sales_df.show()\n",
        "product_df = spark.createDataFrame(product_data, product_columns)\n",
        "product_df.show()\n",
        "# Perform inner join on product_id\n",
        "joined_df = sales_df.join(product_df, on=\"product_id\", how=\"inner\")\n",
        "joined_df.show()\n",
        "# Select required columns\n",
        "result_df = joined_df.select(\"product_name\", \"year\", \"price\")\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvYrxMT8bPHh",
        "outputId": "e38c0dac-0b1e-4681-9939-af039413eb60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+--------+-----+\n",
            "|sale_id|product_id|year|quantity|price|\n",
            "+-------+----------+----+--------+-----+\n",
            "|      1|       100|2008|      10| 5000|\n",
            "|      2|       100|2009|      12| 5000|\n",
            "|      7|       200|2011|      15| 9000|\n",
            "+-------+----------+----+--------+-----+\n",
            "\n",
            "+----------+------------+\n",
            "|product_id|product_name|\n",
            "+----------+------------+\n",
            "|       100|       Nokia|\n",
            "|       200|       Apple|\n",
            "|       300|     Samsung|\n",
            "+----------+------------+\n",
            "\n",
            "+----------+-------+----+--------+-----+------------+\n",
            "|product_id|sale_id|year|quantity|price|product_name|\n",
            "+----------+-------+----+--------+-----+------------+\n",
            "|       100|      1|2008|      10| 5000|       Nokia|\n",
            "|       100|      2|2009|      12| 5000|       Nokia|\n",
            "|       200|      7|2011|      15| 9000|       Apple|\n",
            "+----------+-------+----+--------+-----+------------+\n",
            "\n",
            "+------------+----+-----+\n",
            "|product_name|year|price|\n",
            "+------------+----+-----+\n",
            "|       Nokia|2008| 5000|\n",
            "|       Nokia|2009| 5000|\n",
            "|       Apple|2011| 9000|\n",
            "+------------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"VisitsWithoutTransactions\").getOrCreate()\n",
        "\n",
        "# Visits data\n",
        "visits_data = [\n",
        "    (1, 23),\n",
        "    (2, 9),\n",
        "    (4, 30),\n",
        "    (5, 54),\n",
        "    (6, 96),\n",
        "    (7, 54),\n",
        "    (8, 54)\n",
        "]\n",
        "visits_columns = [\"visit_id\", \"customer_id\"]\n",
        "\n",
        "# Transactions data\n",
        "transactions_data = [\n",
        "    (2, 5, 310),\n",
        "    (3, 5, 300),\n",
        "    (9, 5, 200),\n",
        "    (12, 1, 910),\n",
        "    (13, 2, 970)\n",
        "]\n",
        "transactions_columns = [\"transaction_id\", \"visit_id\", \"amount\"]\n",
        "\n",
        "# Create DataFrames\n",
        "visits_df = spark.createDataFrame(visits_data, visits_columns)\n",
        "visits_df.show()\n",
        "transactions_df = spark.createDataFrame(transactions_data, transactions_columns)\n",
        "transactions_df.show()\n",
        "# Left join Visits with Transactions\n",
        "joined_df = visits_df.join(transactions_df, on=\"visit_id\", how=\"left\")\n",
        "joined_df.show()\n",
        "# Filter visits with no transaction\n",
        "no_trans_df = joined_df.filter(col(\"transaction_id\").isNull())\n",
        "no_trans_df.show()\n",
        "# Group by customer_id and count\n",
        "result_df = no_trans_df.groupBy(\"customer_id\").agg(count(\"*\").alias(\"count_no_trans\"))\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13fS05i9cQEK",
        "outputId": "013daf9b-8b2d-42fa-8972-938df76103ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|visit_id|customer_id|\n",
            "+--------+-----------+\n",
            "|       1|         23|\n",
            "|       2|          9|\n",
            "|       4|         30|\n",
            "|       5|         54|\n",
            "|       6|         96|\n",
            "|       7|         54|\n",
            "|       8|         54|\n",
            "+--------+-----------+\n",
            "\n",
            "+--------------+--------+------+\n",
            "|transaction_id|visit_id|amount|\n",
            "+--------------+--------+------+\n",
            "|             2|       5|   310|\n",
            "|             3|       5|   300|\n",
            "|             9|       5|   200|\n",
            "|            12|       1|   910|\n",
            "|            13|       2|   970|\n",
            "+--------------+--------+------+\n",
            "\n",
            "+--------+-----------+--------------+------+\n",
            "|visit_id|customer_id|transaction_id|amount|\n",
            "+--------+-----------+--------------+------+\n",
            "|       1|         23|            12|   910|\n",
            "|       2|          9|            13|   970|\n",
            "|       4|         30|          NULL|  NULL|\n",
            "|       7|         54|          NULL|  NULL|\n",
            "|       6|         96|          NULL|  NULL|\n",
            "|       5|         54|             9|   200|\n",
            "|       5|         54|             3|   300|\n",
            "|       5|         54|             2|   310|\n",
            "|       8|         54|          NULL|  NULL|\n",
            "+--------+-----------+--------------+------+\n",
            "\n",
            "+--------+-----------+--------------+------+\n",
            "|visit_id|customer_id|transaction_id|amount|\n",
            "+--------+-----------+--------------+------+\n",
            "|       4|         30|          NULL|  NULL|\n",
            "|       7|         54|          NULL|  NULL|\n",
            "|       6|         96|          NULL|  NULL|\n",
            "|       8|         54|          NULL|  NULL|\n",
            "+--------+-----------+--------------+------+\n",
            "\n",
            "+-----------+--------------+\n",
            "|customer_id|count_no_trans|\n",
            "+-----------+--------------+\n",
            "|         54|             2|\n",
            "|         96|             1|\n",
            "|         30|             1|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, lag\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"RisingTemperature\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "weather_data = [\n",
        "    (1, \"2015-01-01\", 10),\n",
        "    (2, \"2015-01-02\", 25),\n",
        "    (3, \"2015-01-03\", 20),\n",
        "    (4, \"2015-01-04\", 30)\n",
        "]\n",
        "weather_columns = [\"id\", \"recordDate\", \"temperature\"]\n",
        "\n",
        "# Create DataFrame\n",
        "weather_df = spark.createDataFrame(weather_data, weather_columns)\n",
        "weather_df.show()\n",
        "# Define window spec ordered by date\n",
        "window_spec = Window.orderBy(\"recordDate\")\n",
        "\n",
        "# Add previous day's temperature\n",
        "weather_with_lag = weather_df.withColumn(\"prev_temp\", lag(\"temperature\").over(window_spec))\n",
        "\n",
        "# Filter where current temp > previous temp\n",
        "result_df = weather_with_lag.filter(col(\"temperature\") > col(\"prev_temp\")).select(\"id\")\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPCNyMb4fo63",
        "outputId": "9c76dbc4-b5cc-4ea0-bd5e-34c9dcbbe69d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----------+\n",
            "| id|recordDate|temperature|\n",
            "+---+----------+-----------+\n",
            "|  1|2015-01-01|         10|\n",
            "|  2|2015-01-02|         25|\n",
            "|  3|2015-01-03|         20|\n",
            "|  4|2015-01-04|         30|\n",
            "+---+----------+-----------+\n",
            "\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  2|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (0, \"06-26-2011\", 300.4, \"Exercise\", \"GymnasticsPro\", \"cash\"),\n",
        "    (1, \"05-26-2011\", 200.0, \"Exercise Band\", \"Weightlifting\", \"credit\"),\n",
        "    (2, \"06-01-2011\", 300.4, \"Exercise\", \"Gymnastics Pro\", \"cash\"),\n",
        "    (3, \"06-05-2011\", 100.0, \"Gymnastics\", \"Rings\", \"credit\"),\n",
        "    (4, \"12-17-2011\", 300.0, \"Team Sports\", \"Field\", \"cash\"),\n",
        "    (5, \"02-14-2011\", 200.0, \"Gymnastics\", None, \"cash\"),\n",
        "    (6, \"06-05-2011\", 100.0, \"Exercise\", \"Rings\", \"credit\"),\n",
        "    (7, \"12-17-2011\", 300.0, \"Team Sports\", \"Field\", \"cash\"),\n",
        "    (8, \"02-14-2011\", 200.0, \"Gymnastics\", None, \"cash\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"tdate\", \"amount\", \"category\", \"product\", \"spendby\"])\n",
        "df.show()\n",
        "\n",
        "\n",
        "data2 = [\n",
        "    (4, \"12-17-2011\", 300.0, \"Team Sports\", \"Field\", \"cash\"),\n",
        "    (5, \"02-14-2011\", 200.0, \"Gymnastics\", None, \"cash\"),\n",
        "    (6, \"02-14-2011\", 200.0, \"Winter\", None, \"cash\"),\n",
        "    (7, \"02-14-2011\", 200.0, \"Winter\", None, \"cash\")\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(data2, [\"id\", \"tdate\", \"amount\", \"category\", \"product\", \"spendby\"])\n",
        "df1.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data4 = [\n",
        "    (1, \"raj\"),\n",
        "    (2, \"ravi\"),\n",
        "    (3, \"sai\"),\n",
        "    (5, \"rani\")\n",
        "]\n",
        "\n",
        "\n",
        "cust = spark.createDataFrame(data4, [\"id\", \"name\"])\n",
        "cust.show()\n",
        "\n",
        "data3 = [\n",
        "    (1, \"mouse\"),\n",
        "    (3, \"mobile\"),\n",
        "    (7, \"laptop\")\n",
        "]\n",
        "\n",
        "prod = spark.createDataFrame(data3, [\"id\", \"product\"])\n",
        "prod.show()\n",
        "\n",
        "# Register DataFrames as temporary views\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "df1.createOrReplaceTempView(\"df1\")\n",
        "cust.createOrReplaceTempView(\"cust\")\n",
        "prod.createOrReplaceTempView(\"prod\")\n",
        "\n",
        "spark.sql(\"select category, lower(category) as lower from df\").show()\n",
        "\n",
        "spark.sql(\"select category, upper(category) as upper from df\").show()\n",
        "\n",
        "spark.sql(\"select amount, ceil(amount) as ceil from df\").show()\n",
        "\n",
        "spark.sql(\"select amount, round(amount) as round from df\").show()\n",
        "\n",
        "spark.sql(\"select product,coalesce(product,'NA') as nullrep from df\").show()\n",
        "\n",
        "spark.sql(\"select trim(product) from df\").show()\n",
        "\n",
        "spark.sql(\"select distinct category from df\").show()\n",
        "\n",
        "spark.sql(\"select distinct category,spendby from df\").show()\n",
        "\n",
        "spark.sql(\"select substring(product,1,10) as sub from df\").show()\n",
        "\n",
        "spark.sql(\"select product,split(product,' ')[0] as split from df\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsgCxwQha6mb",
        "outputId": "42901b09-9127-4041-f381-58a1318e5bef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+-------------+--------------+-------+\n",
            "| id|     tdate|amount|     category|       product|spendby|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|\n",
            "|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|\n",
            "|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|\n",
            "|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  5|02-14-2011| 200.0|   Gymnastics|          NULL|   cash|\n",
            "|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|\n",
            "|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  8|02-14-2011| 200.0|   Gymnastics|          NULL|   cash|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "\n",
            "+---+----------+------+-----------+-------+-------+\n",
            "| id|     tdate|amount|   category|product|spendby|\n",
            "+---+----------+------+-----------+-------+-------+\n",
            "|  4|12-17-2011| 300.0|Team Sports|  Field|   cash|\n",
            "|  5|02-14-2011| 200.0| Gymnastics|   NULL|   cash|\n",
            "|  6|02-14-2011| 200.0|     Winter|   NULL|   cash|\n",
            "|  7|02-14-2011| 200.0|     Winter|   NULL|   cash|\n",
            "+---+----------+------+-----------+-------+-------+\n",
            "\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| raj|\n",
            "|  2|ravi|\n",
            "|  3| sai|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n",
            "+---+-------+\n",
            "| id|product|\n",
            "+---+-------+\n",
            "|  1|  mouse|\n",
            "|  3| mobile|\n",
            "|  7| laptop|\n",
            "+---+-------+\n",
            "\n",
            "+-------------+-------------+\n",
            "|     category|        lower|\n",
            "+-------------+-------------+\n",
            "|     Exercise|     exercise|\n",
            "|Exercise Band|exercise band|\n",
            "|     Exercise|     exercise|\n",
            "|   Gymnastics|   gymnastics|\n",
            "|  Team Sports|  team sports|\n",
            "|   Gymnastics|   gymnastics|\n",
            "|     Exercise|     exercise|\n",
            "|  Team Sports|  team sports|\n",
            "|   Gymnastics|   gymnastics|\n",
            "+-------------+-------------+\n",
            "\n",
            "+-------------+-------------+\n",
            "|     category|        upper|\n",
            "+-------------+-------------+\n",
            "|     Exercise|     EXERCISE|\n",
            "|Exercise Band|EXERCISE BAND|\n",
            "|     Exercise|     EXERCISE|\n",
            "|   Gymnastics|   GYMNASTICS|\n",
            "|  Team Sports|  TEAM SPORTS|\n",
            "|   Gymnastics|   GYMNASTICS|\n",
            "|     Exercise|     EXERCISE|\n",
            "|  Team Sports|  TEAM SPORTS|\n",
            "|   Gymnastics|   GYMNASTICS|\n",
            "+-------------+-------------+\n",
            "\n",
            "+------+----+\n",
            "|amount|ceil|\n",
            "+------+----+\n",
            "| 300.4| 301|\n",
            "| 200.0| 200|\n",
            "| 300.4| 301|\n",
            "| 100.0| 100|\n",
            "| 300.0| 300|\n",
            "| 200.0| 200|\n",
            "| 100.0| 100|\n",
            "| 300.0| 300|\n",
            "| 200.0| 200|\n",
            "+------+----+\n",
            "\n",
            "+------+-----+\n",
            "|amount|round|\n",
            "+------+-----+\n",
            "| 300.4|300.0|\n",
            "| 200.0|200.0|\n",
            "| 300.4|300.0|\n",
            "| 100.0|100.0|\n",
            "| 300.0|300.0|\n",
            "| 200.0|200.0|\n",
            "| 100.0|100.0|\n",
            "| 300.0|300.0|\n",
            "| 200.0|200.0|\n",
            "+------+-----+\n",
            "\n",
            "+--------------+--------------+\n",
            "|       product|       nullrep|\n",
            "+--------------+--------------+\n",
            "| GymnasticsPro| GymnasticsPro|\n",
            "| Weightlifting| Weightlifting|\n",
            "|Gymnastics Pro|Gymnastics Pro|\n",
            "|         Rings|         Rings|\n",
            "|         Field|         Field|\n",
            "|          NULL|            NA|\n",
            "|         Rings|         Rings|\n",
            "|         Field|         Field|\n",
            "|          NULL|            NA|\n",
            "+--------------+--------------+\n",
            "\n",
            "+--------------+\n",
            "| trim(product)|\n",
            "+--------------+\n",
            "| GymnasticsPro|\n",
            "| Weightlifting|\n",
            "|Gymnastics Pro|\n",
            "|         Rings|\n",
            "|         Field|\n",
            "|          NULL|\n",
            "|         Rings|\n",
            "|         Field|\n",
            "|          NULL|\n",
            "+--------------+\n",
            "\n",
            "+-------------+\n",
            "|     category|\n",
            "+-------------+\n",
            "|   Gymnastics|\n",
            "|     Exercise|\n",
            "|Exercise Band|\n",
            "|  Team Sports|\n",
            "+-------------+\n",
            "\n",
            "+-------------+-------+\n",
            "|     category|spendby|\n",
            "+-------------+-------+\n",
            "|     Exercise|   cash|\n",
            "|Exercise Band| credit|\n",
            "|   Gymnastics| credit|\n",
            "|   Gymnastics|   cash|\n",
            "|     Exercise| credit|\n",
            "|  Team Sports|   cash|\n",
            "+-------------+-------+\n",
            "\n",
            "+----------+\n",
            "|       sub|\n",
            "+----------+\n",
            "|Gymnastics|\n",
            "|Weightlift|\n",
            "|Gymnastics|\n",
            "|     Rings|\n",
            "|     Field|\n",
            "|      NULL|\n",
            "|     Rings|\n",
            "|     Field|\n",
            "|      NULL|\n",
            "+----------+\n",
            "\n",
            "+--------------+-------------+\n",
            "|       product|        split|\n",
            "+--------------+-------------+\n",
            "| GymnasticsPro|GymnasticsPro|\n",
            "| Weightlifting|Weightlifting|\n",
            "|Gymnastics Pro|   Gymnastics|\n",
            "|         Rings|        Rings|\n",
            "|         Field|        Field|\n",
            "|          NULL|         NULL|\n",
            "|         Rings|        Rings|\n",
            "|         Field|        Field|\n",
            "|          NULL|         NULL|\n",
            "+--------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from df union all select * from df1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOseIc9VeX7s",
        "outputId": "c271e5b7-d517-4847-d156-00f2e9427171"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+-------------+--------------+-------+\n",
            "| id|     tdate|amount|     category|       product|spendby|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|\n",
            "|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|\n",
            "|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|\n",
            "|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  5|02-14-2011| 200.0|   Gymnastics|          NULL|   cash|\n",
            "|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|\n",
            "|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  8|02-14-2011| 200.0|   Gymnastics|          NULL|   cash|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  5|02-14-2011| 200.0|   Gymnastics|          NULL|   cash|\n",
            "|  6|02-14-2011| 200.0|       Winter|          NULL|   cash|\n",
            "|  7|02-14-2011| 200.0|       Winter|          NULL|   cash|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from df union select * from df1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQptLGK0iDtX",
        "outputId": "93ea6144-3382-4e9c-90ab-b86aeb53b0db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+-------------+--------------+-------+\n",
            "| id|     tdate|amount|     category|       product|spendby|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "|  0|06-26-2011| 300.4|     Exercise| GymnasticsPro|   cash|\n",
            "|  3|06-05-2011| 100.0|   Gymnastics|         Rings| credit|\n",
            "|  2|06-01-2011| 300.4|     Exercise|Gymnastics Pro|   cash|\n",
            "|  1|05-26-2011| 200.0|Exercise Band| Weightlifting| credit|\n",
            "|  7|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  6|06-05-2011| 100.0|     Exercise|         Rings| credit|\n",
            "|  4|12-17-2011| 300.0|  Team Sports|         Field|   cash|\n",
            "|  8|02-14-2011| 200.0|   Gymnastics|          NULL|   cash|\n",
            "|  5|02-14-2011| 200.0|   Gymnastics|          NULL|   cash|\n",
            "|  6|02-14-2011| 200.0|       Winter|          NULL|   cash|\n",
            "|  7|02-14-2011| 200.0|       Winter|          NULL|   cash|\n",
            "+---+----------+------+-------------+--------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select category,sum(amount) as sum from df group by category\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uScgMUDkk-gx",
        "outputId": "3c89a0b4-8994-4701-b949-a5069973def8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|     category|  sum|\n",
            "+-------------+-----+\n",
            "|   Gymnastics|500.0|\n",
            "|     Exercise|700.8|\n",
            "|Exercise Band|200.0|\n",
            "|  Team Sports|600.0|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "\n",
        "    (\"00000\", \"06-26-2011\", 200, \"Exercise\", \"GymnasticsPro\", \"cash\"),\n",
        "    (\"00001\", \"05-26-2011\", 300, \"Exercise\", \"Weightlifting\", \"credit\"),\n",
        "    (\"00002\", \"06-01-2011\", 100, \"Exercise\", \"GymnasticsPro\", \"cash\"),\n",
        "    (\"00003\", \"06-05-2011\", 100, \"Gymnastics\", \"Rings\", \"credit\"),\n",
        "    (\"00004\", \"12-17-2011\", 300, \"Team Sports\", \"Field\", \"paytm\"),\n",
        "    (\"00005\", \"02-14-2011\", 200, \"Gymnastics\", None , \"cash\")\n",
        "\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"tdate\", \"amount\", \"category\", \"product\", \"spendby\"])\n",
        "df.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "procdf = df.selectExpr(\n",
        "\n",
        "         \"cast(id as int) as  id\",\n",
        "\n",
        "               \"year(from_unixtime(unix_timestamp(tdate,'MM-dd-yyyy')))  as year\",\n",
        "\n",
        "               \"amount + 1000  as amount\",\n",
        "\n",
        "               \"upper(category) as category\",\n",
        "\n",
        "               \"concat(product,'~zeyo') as product\",\n",
        "\n",
        "               \"spendby\",\n",
        "\n",
        "               \"\"\" case\n",
        "                    when spendby='cash'  then 0\n",
        "                    when spendby='credit'  then 1\n",
        "                    else 2\n",
        "                    end\n",
        "                    as status\n",
        "                \"\"\"\n",
        "\n",
        ")\n",
        "\n",
        "procdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v04jpZ2-7_7c",
        "outputId": "ebdae443-f334-4f2e-c57f-e849566e8d73"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|   id|     tdate|amount|   category|      product|spendby|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|00000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|\n",
            "|00001|05-26-2011|   300|   Exercise|Weightlifting| credit|\n",
            "|00002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|\n",
            "|00003|06-05-2011|   100| Gymnastics|        Rings| credit|\n",
            "|00004|12-17-2011|   300|Team Sports|        Field|  paytm|\n",
            "|00005|02-14-2011|   200| Gymnastics|         NULL|   cash|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "| id|year|amount|   category|           product|spendby|status|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "|  0|2011|  1200|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  1|2011|  1300|   EXERCISE|Weightlifting~zeyo| credit|     1|\n",
            "|  2|2011|  1100|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  3|2011|  1100| GYMNASTICS|        Rings~zeyo| credit|     1|\n",
            "|  4|2011|  1300|TEAM SPORTS|        Field~zeyo|  paytm|     2|\n",
            "|  5|2011|  1200| GYMNASTICS|              NULL|   cash|     0|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "\n",
        "    (\"00000\", \"06-26-2011\", 200, \"Exercise\", \"GymnasticsPro\", \"cash\"),\n",
        "    (\"00001\", \"05-26-2011\", 300, \"Exercise\", \"Weightlifting\", \"credit\"),\n",
        "    (\"00002\", \"06-01-2011\", 100, \"Exercise\", \"GymnasticsPro\", \"cash\"),\n",
        "    (\"00003\", \"06-05-2011\", 100, \"Gymnastics\", \"Rings\", \"credit\"),\n",
        "    (\"00004\", \"12-17-2011\", 300, \"Team Sports\", \"Field\", \"paytm\"),\n",
        "    (\"00005\", \"02-14-2011\", 200, \"Gymnastics\", None , \"cash\")\n",
        "\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"tdate\", \"amount\", \"category\", \"product\", \"spendby\"])\n",
        "df.show()\n",
        "\n",
        "\n",
        "\n",
        "withdf = (\n",
        "\n",
        "            df.withColumn(\"category\",expr(\"upper(category)\"))\n",
        "              .withColumn(\"amount\",expr(\"amount+1000\"))\n",
        "              .withColumn(\"product\",expr(\"concat(product,'~zeyo')\"))\n",
        "              .withColumn(\"id\",expr(\"cast(id as int)\"))\n",
        "              .withColumn(\"tdate\",expr(\"split(tdate,'-')[2]\"))\n",
        "              .withColumn(\"status\",expr(\"case when spendby='cash' then 0 else 1 end as newstatus\"))\n",
        "              .withColumnRenamed(\"tdate\",\"year\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "withdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_Xiu0O9QClx",
        "outputId": "601eafdf-b93f-4032-b03e-bfe4e2f51009"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|   id|     tdate|amount|   category|      product|spendby|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|00000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|\n",
            "|00001|05-26-2011|   300|   Exercise|Weightlifting| credit|\n",
            "|00002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|\n",
            "|00003|06-05-2011|   100| Gymnastics|        Rings| credit|\n",
            "|00004|12-17-2011|   300|Team Sports|        Field|  paytm|\n",
            "|00005|02-14-2011|   200| Gymnastics|         NULL|   cash|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "| id|year|amount|   category|           product|spendby|status|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "|  0|2011|  1200|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  1|2011|  1300|   EXERCISE|Weightlifting~zeyo| credit|     1|\n",
            "|  2|2011|  1100|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  3|2011|  1100| GYMNASTICS|        Rings~zeyo| credit|     1|\n",
            "|  4|2011|  1300|TEAM SPORTS|        Field~zeyo|  paytm|     1|\n",
            "|  5|2011|  1200| GYMNASTICS|              NULL|   cash|     0|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data4 = [\n",
        "    (1, \"raj\"),\n",
        "    (2, \"ravi\"),\n",
        "    (3, \"sai\"),\n",
        "    (5, \"rani\")\n",
        "]\n",
        "\n",
        "cust = spark.createDataFrame(data4, [\"id\", \"name\"])\n",
        "cust.show()\n",
        "data3 = [\n",
        "    (1, \"mouse\"),\n",
        "    (3, \"mobile\"),\n",
        "    (7, \"laptop\")\n",
        "]\n",
        "prod = spark.createDataFrame(data3, [\"id\", \"product\"])\n",
        "prod.show()\n",
        "\n",
        "\n",
        "innerjoin = cust.join(  prod , [\"id\"], \"inner\")\n",
        "innerjoin.show()\n",
        "\n",
        "lefjoin  =   cust.join(prod, [\"id\"], \"left\")\n",
        "lefjoin.show()\n",
        "\n",
        "rightjoin = cust.join(prod, [\"id\"], \"right\")\n",
        "rightjoin.show()\n",
        "\n",
        "fulljoin = cust.join(prod, [\"id\"],\"full\")\n",
        "fulljoin.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf25H4m7ZTaD",
        "outputId": "1bf804a0-d5e5-4af4-d616-e3ff781413cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| raj|\n",
            "|  2|ravi|\n",
            "|  3| sai|\n",
            "|  5|rani|\n",
            "+---+----+\n",
            "\n",
            "+---+-------+\n",
            "| id|product|\n",
            "+---+-------+\n",
            "|  1|  mouse|\n",
            "|  3| mobile|\n",
            "|  7| laptop|\n",
            "+---+-------+\n",
            "\n",
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  3| sai| mobile|\n",
            "+---+----+-------+\n",
            "\n",
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  2|ravi|   NULL|\n",
            "|  5|rani|   NULL|\n",
            "|  3| sai| mobile|\n",
            "+---+----+-------+\n",
            "\n",
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  7|NULL| laptop|\n",
            "|  3| sai| mobile|\n",
            "+---+----+-------+\n",
            "\n",
            "+---+----+-------+\n",
            "| id|name|product|\n",
            "+---+----+-------+\n",
            "|  1| raj|  mouse|\n",
            "|  2|ravi|   NULL|\n",
            "|  3| sai| mobile|\n",
            "|  5|rani|   NULL|\n",
            "|  7|NULL| laptop|\n",
            "+---+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"TransformData\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"00000\",\"06-26-2011\",200,\"Exercise\",\"GymnasticsPro\",\"cash\"),\n",
        "    (\"00001\",\"05-26-2011\",300,\"Exercise\",\"Weightlifting\",\"credit\"),\n",
        "    (\"00002\",\"06-01-2011\",100,\"Exercise\",\"GymnasticsPro\",\"cash\"),\n",
        "    (\"00003\",\"06-05-2011\",300,\"Gymnastics\",\"Rings\",\"credit\"),\n",
        "    (\"00004\",\"12-17-2011\",300,\"Team Sports\",\"Field\",\"paytm\"),\n",
        "    (\"00005\",\"02-14-2011\",200,\"Gymnastics\",None,\"cash\")\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"id\", \"tdate\", \"amount\", \"category\", \"product\", \"spendby\"])\n",
        "df.show()\n",
        "\n",
        "# Apply transformations\n",
        "withdf = (\n",
        "    df.withColumn(\"category\", expr(\"upper(category)\"))\n",
        "      .withColumn(\"amount\", expr(\"amount + 1000\"))\n",
        "      .withColumn(\"product\", expr(\"concat(product, '~zeyo')\"))\n",
        "      .withColumn(\"id\", expr(\"cast(id as int)\"))\n",
        "      .withColumn(\"tdate\", expr(\"split(tdate, '-')[2]\"))\n",
        "      .withColumn(\"status\", expr(\"case when spendby = 'cash' then 0 else 1 end\"))\n",
        "      .withColumnRenamed(\"tdate\", \"year\")\n",
        ")\n",
        "\n",
        "# Show transformed DataFrame\n",
        "withdf.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54RxGR14Stpj",
        "outputId": "9d9f2919-f935-4f5f-e3dd-85236b54fccb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|   id|     tdate|amount|   category|      product|spendby|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|00000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|\n",
            "|00001|05-26-2011|   300|   Exercise|Weightlifting| credit|\n",
            "|00002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|\n",
            "|00003|06-05-2011|   300| Gymnastics|        Rings| credit|\n",
            "|00004|12-17-2011|   300|Team Sports|        Field|  paytm|\n",
            "|00005|02-14-2011|   200| Gymnastics|         NULL|   cash|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "| id|year|amount|   category|           product|spendby|status|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "|  0|2011|  1200|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  1|2011|  1300|   EXERCISE|Weightlifting~zeyo| credit|     1|\n",
            "|  2|2011|  1100|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  3|2011|  1300| GYMNASTICS|        Rings~zeyo| credit|     1|\n",
            "|  4|2011|  1300|TEAM SPORTS|        Field~zeyo|  paytm|     1|\n",
            "|  5|2011|  1200| GYMNASTICS|              NULL|   cash|     0|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "     (\"00000\",\"06-26-2011\",200,\"Exercise\",\"GymnasticsPro\",\"cash\"),\n",
        "     (\"00001\",\"05-26-2011\",300,\"Exercise\",\"Weightlifting\",\"credit\"),\n",
        "     (\"00002\",\"06-01-2011\",100,\"Exercise\",\"GymnasticsPro\",\"cash\"),\n",
        "     (\"00003\",\"06-05-2011\",300,\"Gymnastics\",\"Rings\",\"credit\"),\n",
        "     (\"00004\",\"12-17-2011\",300,\"Team Sports\",\"Field\",\"paytm\"),\n",
        "     (\"00005\",\"02-14-2011\",200,\"Gymnastics\",None,\"cash\")\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"tdate\", \"amount\", \"category\", \"product\", \"spendby\"])\n",
        "df.show()\n",
        "\n",
        "\n",
        "withdf = (\n",
        "\t\tdf.withColumn(\"category\",expr(\"upper(category)\"))\n",
        "\t\t  .withColumn(\"amount\",expr(\"amount+1000\"))\n",
        "\t\t  .withColumn(\"product\",expr(\"concat(product, '~zeyo')\"))\n",
        "\t\t  .withColumn(\"id\",expr(\"cast(id as int)\"))\n",
        "\t\t  .withColumn(\"tdate\",expr(\"split(tdate,'-')[2]\"))\n",
        "\t\t  .withColumn(\"status\",expr(\"case when spendby='cash' then 0 else 1 end as newstatus\"))\n",
        "\t\t  .withColumnRenamed(\"tdate\",\"year\")\n",
        ")\n",
        "\n",
        "withdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpki2fWpT51O",
        "outputId": "ad419380-ce82-41c0-ea51-9865f856a285"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|   id|     tdate|amount|   category|      product|spendby|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "|00000|06-26-2011|   200|   Exercise|GymnasticsPro|   cash|\n",
            "|00001|05-26-2011|   300|   Exercise|Weightlifting| credit|\n",
            "|00002|06-01-2011|   100|   Exercise|GymnasticsPro|   cash|\n",
            "|00003|06-05-2011|   300| Gymnastics|        Rings| credit|\n",
            "|00004|12-17-2011|   300|Team Sports|        Field|  paytm|\n",
            "|00005|02-14-2011|   200| Gymnastics|         NULL|   cash|\n",
            "+-----+----------+------+-----------+-------------+-------+\n",
            "\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "| id|year|amount|   category|           product|spendby|status|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "|  0|2011|  1200|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  1|2011|  1300|   EXERCISE|Weightlifting~zeyo| credit|     1|\n",
            "|  2|2011|  1100|   EXERCISE|GymnasticsPro~zeyo|   cash|     0|\n",
            "|  3|2011|  1300| GYMNASTICS|        Rings~zeyo| credit|     1|\n",
            "|  4|2011|  1300|TEAM SPORTS|        Field~zeyo|  paytm|     1|\n",
            "|  5|2011|  1200| GYMNASTICS|              NULL|   cash|     0|\n",
            "+---+----+------+-----------+------------------+-------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}