{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTZKh7i7I+5UVXN2Q2Ru1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PendlimarriSivasankar/PS/blob/main/Pyspark_Scenario_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lnSdJWno2Nd_"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"colab pyspark\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lead, col, expr\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "data = [(1111, \"2021-01-15\", 10),\n",
        "        (1111, \"2021-01-16\", 15),\n",
        "        (1111, \"2021-01-17\", 30),\n",
        "        (1112, \"2021-01-15\", 10),\n",
        "        (1112, \"2021-01-15\", 20),\n",
        "        (1112, \"2021-01-15\", 30)]\n",
        "\n",
        "myschema = [\"sensorid\", \"timestamp\", \"values\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=myschema)\n",
        "df.show()\n",
        "d1 = Window.partitionBy(\"sensorid\").orderBy(\"values\")\n",
        "\n",
        "finaldf = df.withColumn(\"nextvalues\", lead(\"values\", 1).over(d1)) \\\n",
        "    .filter(col(\"nextvalues\").isNotNull()) \\\n",
        "    .withColumn(\"values\", expr(\"nextvalues-values\")) \\\n",
        "    .drop(\"nextvalues\") \\\n",
        "    .orderBy(col(\"sensorid\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T4KZ3kA6jat",
        "outputId": "02760569-6f68-49b8-b637-53cbcf48178f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|sensorid| timestamp|values|\n",
            "+--------+----------+------+\n",
            "|    1111|2021-01-15|    10|\n",
            "|    1111|2021-01-16|    15|\n",
            "|    1111|2021-01-17|    30|\n",
            "|    1112|2021-01-15|    10|\n",
            "|    1112|2021-01-15|    20|\n",
            "|    1112|2021-01-15|    30|\n",
            "+--------+----------+------+\n",
            "\n",
            "+--------+----------+------+\n",
            "|sensorid| timestamp|values|\n",
            "+--------+----------+------+\n",
            "|    1111|2021-01-15|     5|\n",
            "|    1111|2021-01-16|    15|\n",
            "|    1112|2021-01-15|    10|\n",
            "|    1112|2021-01-15|    10|\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "Person = [\n",
        "    (1, \"Wang\", \"Allen\"),\n",
        "    (2, \"Alice\", \"Bob\")\n",
        "]\n",
        "\n",
        "Person_columns = [\"personId\", \"lastName\", \"firstName\"]\n",
        "Person_df = spark.createDataFrame(Person, Person_columns)\n",
        "Person_df.show()\n",
        "\n",
        "address = [\n",
        "    (1, 2, \"New York City\", \"New York\"),\n",
        "    (2, 3, \"Leetcode\", \"California\")\n",
        "]\n",
        "\n",
        "address_columns = [\"addressId\", \"personId\", \"city\", \"state\"]\n",
        "address_df = spark.createDataFrame(address, address_columns)\n",
        "address_df.show()\n",
        "\n",
        "Person = [\n",
        "    (1, \"Wang\", \"Allen\"),\n",
        "    (2, \"Alice\", \"Bob\")\n",
        "]\n",
        "\n",
        "Person_columns = [\"personId\", \"lastName\", \"firstName\"]\n",
        "Person_df = spark.createDataFrame(Person, Person_columns)\n",
        "Person_df.show()\n",
        "\n",
        "address = [\n",
        "    (1, 2, \"New York City\", \"New York\"),\n",
        "    (2, 3, \"Leetcode\", \"California\")\n",
        "]\n",
        "\n",
        "address_columns = [\"addressId\", \"personId\", \"city\", \"state\"]\n",
        "address_df = spark.createDataFrame(address, address_columns)\n",
        "address_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2oIqpkYpkKd",
        "outputId": "140037c4-0f51-435b-d8ec-459f7b89bd4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+---------+\n",
            "|personId|lastName|firstName|\n",
            "+--------+--------+---------+\n",
            "|       1|    Wang|    Allen|\n",
            "|       2|   Alice|      Bob|\n",
            "+--------+--------+---------+\n",
            "\n",
            "+---------+--------+-------------+----------+\n",
            "|addressId|personId|         city|     state|\n",
            "+---------+--------+-------------+----------+\n",
            "|        1|       2|New York City|  New York|\n",
            "|        2|       3|     Leetcode|California|\n",
            "+---------+--------+-------------+----------+\n",
            "\n",
            "+--------+--------+---------+\n",
            "|personId|lastName|firstName|\n",
            "+--------+--------+---------+\n",
            "|       1|    Wang|    Allen|\n",
            "|       2|   Alice|      Bob|\n",
            "+--------+--------+---------+\n",
            "\n",
            "+---------+--------+-------------+----------+\n",
            "|addressId|personId|         city|     state|\n",
            "+---------+--------+-------------+----------+\n",
            "|        1|       2|New York City|  New York|\n",
            "|        2|       3|     Leetcode|California|\n",
            "+---------+--------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data = [\n",
        "    (1, 5),\n",
        "    (2, 6),\n",
        "    (3, 5),\n",
        "    (3, 6),\n",
        "    (1, 6)\n",
        "]\n",
        "\n",
        "customer_columns = [\"customer_id\", \"product_key\"]\n",
        "customer_df = spark.createDataFrame(customer_data, customer_columns)\n",
        "customer_df.show()\n",
        "\n",
        "\n",
        "product_data = [\n",
        "    (5,),\n",
        "    (6,)\n",
        "]\n",
        "product_columns = [\"product_key\"]\n",
        "product_df = spark.createDataFrame(product_data, product_columns)\n",
        "product_df.show()\n",
        "total_products = product_df.select(countDistinct(\"product_key\")).collect()[0][0]\n",
        "customer_df=customer_df\\\n",
        "    .groupBy(\"customer_id\").agg(\n",
        "    countDistinct(\"product_key\").alias(\"num_products_bought\"))\\\n",
        "    .filter(\n",
        "        col(\"num_products_bought\") == total_products\n",
        "    ).select(\"customer_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIgvlwVzq7Cp",
        "outputId": "12e9235d-12c3-431a-9b50-6e170a06868d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|customer_id|product_key|\n",
            "+-----------+-----------+\n",
            "|          1|          5|\n",
            "|          2|          6|\n",
            "|          3|          5|\n",
            "|          3|          6|\n",
            "|          1|          6|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+\n",
            "|product_key|\n",
            "+-----------+\n",
            "|          5|\n",
            "|          6|\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|          1|\n",
            "|          3|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actor_director_data = [\n",
        "    (1, 1, 0),\n",
        "    (1, 1, 1),\n",
        "    (1, 1, 2),\n",
        "    (1, 2, 3),\n",
        "    (1, 2, 4),\n",
        "    (2, 1, 5),\n",
        "    (2, 1, 6)\n",
        "]\n",
        "\n",
        "actor_director_columns = [\"actor_id\", \"director_id\", \"timestamp\"]\n",
        "actor_director_df = spark.createDataFrame(actor_director_data, actor_director_columns)\n",
        "actor_director_df.show()\n",
        "actor_director_df=actor_director_df\\\n",
        "    .groupBy(\"actor_id\", \"director_id\")\\\n",
        "    .agg(count(\"*\").alias(\"cooperations\"))\\\n",
        "    .filter(col(\"cooperations\") >= 3)\\\n",
        "    .select(\"actor_id\", \"director_id\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3vM24irrcGv",
        "outputId": "6c1d5281-f98b-467f-f363-7090377cbe63"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+---------+\n",
            "|actor_id|director_id|timestamp|\n",
            "+--------+-----------+---------+\n",
            "|       1|          1|        0|\n",
            "|       1|          1|        1|\n",
            "|       1|          1|        2|\n",
            "|       1|          2|        3|\n",
            "|       1|          2|        4|\n",
            "|       2|          1|        5|\n",
            "|       2|          1|        6|\n",
            "+--------+-----------+---------+\n",
            "\n",
            "+--------+-----------+\n",
            "|actor_id|director_id|\n",
            "+--------+-----------+\n",
            "|       1|          1|\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "    (1, 100, 2008, 10, 5000),\n",
        "    (2, 100, 2009, 12, 5000),\n",
        "    (7, 200, 2011, 15, 9000)\n",
        "]\n",
        "\n",
        "sales_columns = [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"]\n",
        "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
        "sales_df.show()\n",
        "\n",
        "product_data = [\n",
        "    (100, \"Nokia\"),\n",
        "    (200, \"Apple\"),\n",
        "    (300, \"Samsung\")\n",
        "]\n",
        "\n",
        "product_columns = [\"product_id\", \"product_name\"]\n",
        "product_df = spark.createDataFrame(product_data, product_columns)\n",
        "product_df.show()\n",
        "\n",
        "sales_df=sales_df\\\n",
        "    .join(product_df, on=\"product_id\", how=\"inner\")\\\n",
        "    .select(\"product_name\", \"year\", \"price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLlqrs2NswcL",
        "outputId": "42c18cbb-ec3b-4089-b6ac-f855e365d602"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+--------+-----+\n",
            "|sale_id|product_id|year|quantity|price|\n",
            "+-------+----------+----+--------+-----+\n",
            "|      1|       100|2008|      10| 5000|\n",
            "|      2|       100|2009|      12| 5000|\n",
            "|      7|       200|2011|      15| 9000|\n",
            "+-------+----------+----+--------+-----+\n",
            "\n",
            "+----------+------------+\n",
            "|product_id|product_name|\n",
            "+----------+------------+\n",
            "|       100|       Nokia|\n",
            "|       200|       Apple|\n",
            "|       300|     Samsung|\n",
            "+----------+------------+\n",
            "\n",
            "+------------+----+-----+\n",
            "|product_name|year|price|\n",
            "+------------+----+-----+\n",
            "|       Nokia|2008| 5000|\n",
            "|       Nokia|2009| 5000|\n",
            "|       Apple|2011| 9000|\n",
            "+------------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "    (1, 100, 2008, 10, 5000),\n",
        "    (2, 100, 2009, 12, 5000),\n",
        "    (7, 200, 2011, 15, 9000),\n",
        "]\n",
        "\n",
        "sales_columns = [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"]\n",
        "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
        "sales_df.show()\n",
        "\n",
        "product_data = [\n",
        "    (100, \"Nokia\"),\n",
        "    (200, \"Apple\"),\n",
        "    (300, \"Samsung\"),\n",
        "]\n",
        "\n",
        "product_columns = [\"product_id\", \"product_name\"]\n",
        "product_df = spark.createDataFrame(product_data, product_columns)\n",
        "product_df.show()\n",
        "\n",
        "sales_df=sales_df\\\n",
        "    .groupBy(\"product_id\").agg(sum(\"quantity\").alias(\"total_quantity\")).show()"
      ],
      "metadata": {
        "id": "Iea2Ln-6vkoE",
        "outputId": "ea2f7538-f999-43a5-f822-bbe557b45197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+--------+-----+\n",
            "|sale_id|product_id|year|quantity|price|\n",
            "+-------+----------+----+--------+-----+\n",
            "|      1|       100|2008|      10| 5000|\n",
            "|      2|       100|2009|      12| 5000|\n",
            "|      7|       200|2011|      15| 9000|\n",
            "+-------+----------+----+--------+-----+\n",
            "\n",
            "+----------+------------+\n",
            "|product_id|product_name|\n",
            "+----------+------------+\n",
            "|       100|       Nokia|\n",
            "|       200|       Apple|\n",
            "|       300|     Samsung|\n",
            "+----------+------------+\n",
            "\n",
            "+----------+--------------+\n",
            "|product_id|total_quantity|\n",
            "+----------+--------------+\n",
            "|       100|            22|\n",
            "|       200|            15|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "    (1, 100, 2008, 10, 5000),\n",
        "    (2, 100, 2009, 12, 5000),\n",
        "    (7, 200, 2011, 15, 9000)\n",
        "]\n",
        "\n",
        "sales_columns = [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"]\n",
        "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
        "sales_df.show()\n",
        "\n",
        "product_data = [\n",
        "    (100, \"Nokia\"),\n",
        "    (200, \"Apple\"),\n",
        "    (300, \"Samsung\")\n",
        "]\n",
        "\n",
        "product_columns = [\"product_id\", \"product_name\"]\n",
        "product_df = spark.createDataFrame(product_data, product_columns)\n",
        "product_df.show()\n",
        "\n",
        "windowSpec = Window.partitionBy(\"product_id\").orderBy(\"year\")\n",
        "\n",
        "sales_with_rank_df = sales_df.withColumn(\"rn\", row_number().over(windowSpec))\n",
        "\n",
        "sales_with_rank_df=sales_with_rank_df\\\n",
        "    .join(product_df, on=\"product_id\", how=\"inner\")\\\n",
        "        .filter(col(\"rn\") == 1)\\\n",
        "            .select(\"product_name\",col(\"year\").alias(\"first_year\"),\"quantity\",\"price\").show()"
      ],
      "metadata": {
        "id": "yq51uv-iw-DC",
        "outputId": "3527c99c-816b-4e7b-9f55-71507498616e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+--------+-----+\n",
            "|sale_id|product_id|year|quantity|price|\n",
            "+-------+----------+----+--------+-----+\n",
            "|      1|       100|2008|      10| 5000|\n",
            "|      2|       100|2009|      12| 5000|\n",
            "|      7|       200|2011|      15| 9000|\n",
            "+-------+----------+----+--------+-----+\n",
            "\n",
            "+----------+------------+\n",
            "|product_id|product_name|\n",
            "+----------+------------+\n",
            "|       100|       Nokia|\n",
            "|       200|       Apple|\n",
            "|       300|     Samsung|\n",
            "+----------+------------+\n",
            "\n",
            "+------------+----------+--------+-----+\n",
            "|product_name|first_year|quantity|price|\n",
            "+------------+----------+--------+-----+\n",
            "|       Nokia|      2008|      10| 5000|\n",
            "|       Apple|      2011|      15| 9000|\n",
            "+------------+----------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_data = [\n",
        "    (1, 1),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 1),\n",
        "    (2, 4)\n",
        "]\n",
        "\n",
        "project_columns = [\"project_id\", \"employee_id\"]\n",
        "project_df = spark.createDataFrame(project_data, project_columns)\n",
        "project_df.show()\n",
        "\n",
        "employee_data = [\n",
        "    (1, \"Khaled\", 3),\n",
        "    (2, \"Ali\", 2),\n",
        "    (3, \"John\", 1),\n",
        "    (4, \"Doe\", 2)\n",
        "]\n",
        "\n",
        "employee_columns = [\"employee_id\", \"name\", \"experience_years\"]\n",
        "employee_df = spark.createDataFrame(employee_data, employee_columns)\n",
        "employee_df.show()\n",
        "product_df=project_df\\\n",
        "    .join(employee_df, on=\"employee_id\", how=\"inner\")\\\n",
        "        .groupBy(\"project_id\") \\\n",
        "                  .agg(round(avg(\"experience_years\"), 2).alias(\"average_years\")).show()"
      ],
      "metadata": {
        "id": "02CZYDfZxyre",
        "outputId": "96f73641-4af7-466a-ee72-7df89ab268b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|project_id|employee_id|\n",
            "+----------+-----------+\n",
            "|         1|          1|\n",
            "|         1|          2|\n",
            "|         1|          3|\n",
            "|         2|          1|\n",
            "|         2|          4|\n",
            "+----------+-----------+\n",
            "\n",
            "+-----------+------+----------------+\n",
            "|employee_id|  name|experience_years|\n",
            "+-----------+------+----------------+\n",
            "|          1|Khaled|               3|\n",
            "|          2|   Ali|               2|\n",
            "|          3|  John|               1|\n",
            "|          4|   Doe|               2|\n",
            "+-----------+------+----------------+\n",
            "\n",
            "+----------+-------------+\n",
            "|project_id|average_years|\n",
            "+----------+-------------+\n",
            "|         1|          2.0|\n",
            "|         2|          2.5|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_data_1076 = [\n",
        "    (1, 1),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 1),\n",
        "    (2, 4),\n",
        "]\n",
        "project_columns = [\"project_id\", \"employee_id\"]\n",
        "project_df = spark.createDataFrame(project_data, project_columns)\n",
        "project_df.show()\n",
        "\n",
        "employee_data = [\n",
        "    (1, \"Khaled\", 3),\n",
        "    (2, \"Ali\", 2),\n",
        "    (3, \"John\", 1),\n",
        "    (4, \"Doe\", 2),\n",
        "]\n",
        "\n",
        "employee_columns = [\"employee_id\", \"name\", \"experience_years\"]\n",
        "employee_df = spark.createDataFrame(employee_data, employee_columns)\n",
        "employee_df.show()\n",
        "\n",
        "project_counts = project_df.groupBy(\"project_id\") \\\n",
        "                           .agg(count(\"employee_id\").alias(\"employee_count\"))\n",
        "\n",
        "max_count = project_counts.agg(max(\"employee_count\").alias(\"max_count\")).collect()[0][\"max_count\"]\n",
        "\n",
        "project_counts=project_counts.\\\n",
        "    filter(project_counts[\"employee_count\"] == max_count) \\\n",
        "                       .select(\"project_id\").show()"
      ],
      "metadata": {
        "id": "DZXCoZZOz5pN",
        "outputId": "55bb273f-02ce-486d-d795-d7a92b747fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|project_id|employee_id|\n",
            "+----------+-----------+\n",
            "|         1|          1|\n",
            "|         1|          2|\n",
            "|         1|          3|\n",
            "|         2|          1|\n",
            "|         2|          4|\n",
            "+----------+-----------+\n",
            "\n",
            "+-----------+------+----------------+\n",
            "|employee_id|  name|experience_years|\n",
            "+-----------+------+----------------+\n",
            "|          1|Khaled|               3|\n",
            "|          2|   Ali|               2|\n",
            "|          3|  John|               1|\n",
            "|          4|   Doe|               2|\n",
            "+-----------+------+----------------+\n",
            "\n",
            "+----------+\n",
            "|project_id|\n",
            "+----------+\n",
            "|         1|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_data = [\n",
        "    (1, 1),\n",
        "    (1, 2),\n",
        "    (1, 3),\n",
        "    (2, 1),\n",
        "    (2, 4),\n",
        "]\n",
        "project_columns = [\"project_id\", \"employee_id\"]\n",
        "project_df = spark.createDataFrame(project_data, project_columns)\n",
        "project_df.show()\n",
        "\n",
        "employee_data = [\n",
        "    (1, \"Khaled\", 3),\n",
        "    (2, \"Ali\", 2),\n",
        "    (3, \"John\", 3),\n",
        "    (4, \"Doe\", 2),\n",
        "]\n",
        "\n",
        "employee_columns = [\"employee_id\", \"name\", \"experience_years\"]\n",
        "employee_df = spark.createDataFrame(employee_data, employee_columns)\n",
        "employee_df.show()\n",
        "\n",
        "joined_df = project_df.join(employee_df, on=\"employee_id\", how=\"inner\")\n",
        "windowSpec = Window.partitionBy(\"project_id\").orderBy(col(\"experience_years\").desc())\n",
        "\n",
        "joined_df=joined_df\\\n",
        "    .withColumn(\"rnk\", rank().over(windowSpec))\\\n",
        "        .filter(col(\"rnk\") == 1).select(\"project_id\", \"employee_id\").show()"
      ],
      "metadata": {
        "id": "P9VdnXos1Nm7",
        "outputId": "0cc448a6-7928-4c1e-dc12-087d59db9dfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|project_id|employee_id|\n",
            "+----------+-----------+\n",
            "|         1|          1|\n",
            "|         1|          2|\n",
            "|         1|          3|\n",
            "|         2|          1|\n",
            "|         2|          4|\n",
            "+----------+-----------+\n",
            "\n",
            "+-----------+------+----------------+\n",
            "|employee_id|  name|experience_years|\n",
            "+-----------+------+----------------+\n",
            "|          1|Khaled|               3|\n",
            "|          2|   Ali|               2|\n",
            "|          3|  John|               3|\n",
            "|          4|   Doe|               2|\n",
            "+-----------+------+----------------+\n",
            "\n",
            "+----------+-----------+\n",
            "|project_id|employee_id|\n",
            "+----------+-----------+\n",
            "|         1|          1|\n",
            "|         1|          3|\n",
            "|         2|          1|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}