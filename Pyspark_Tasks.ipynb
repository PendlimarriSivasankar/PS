{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PendlimarriSivasankar/PS/blob/main/Pyspark_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX8ON-UvWNCZ",
        "outputId": "12b71c70-1105-4662-cd53-daab553cf8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE==== START YOUR WORKðŸ‘‡ \n"
          ]
        }
      ],
      "source": [
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/df.csv -O df.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/df1.csv -O df1.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/dt.txt -O dt.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file1.txt -O file1.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file2.txt -O file2.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file3.txt -O file3.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file4.json -O file4.json\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file5.parquet -O file5.parquet\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/file6 -O file6\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/prod.csv -O prod.csv\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/state.txt -O state.txt\n",
        "!wget -q https://github.com/saiadityaus1/test1/raw/main/usdata.csv -O usdata.csv\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import sys\n",
        "import os\n",
        "python_path = sys.executable\n",
        "os.environ['PYSPARK_PYTHON'] = python_path\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "conf = SparkConf().setAppName(\"pyspark\").setMaster(\"local[*]\").set(\"spark.driver.host\",\"localhost\").set(\"spark.driver.allowMultipleContexts\",\"true\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.config(\"spark.driver.allowMultipleContexts\",\"true\").getOrCreate()\n",
        "print(\"DONE==== START YOUR WORKðŸ‘‡ \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMv9oRrZYQNg",
        "outputId": "4d6062ba-0629-4d23-dabb-e0461d0e5804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- product_id: long (nullable = true)\n",
            " |-- low_fats: string (nullable = true)\n",
            " |-- recyclable: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Pandas Schema\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = [['0', 'Y', 'N'], ['1', 'Y', 'Y'], ['2', 'N', 'Y'], ['3', 'Y', 'Y'], ['4', 'N', 'N']]\n",
        "products = pd.DataFrame(data, columns=['product_id', 'low_fats', 'recyclable']).astype(\n",
        "    {'product_id': 'int64', 'low_fats': 'category', 'recyclable': 'category'})\n",
        "#converting to spark dataframe\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "products_df = spark.createDataFrame(products)\n",
        "products_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Q6BWIiZI-t",
        "outputId": "608c5ae1-7568-4b7e-a531-ae2f619e56db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+\n",
            "|product_id|low_fats|recyclable|\n",
            "+----------+--------+----------+\n",
            "|         0|       Y|         N|\n",
            "|         1|       Y|         Y|\n",
            "|         2|       N|         Y|\n",
            "|         3|       Y|         Y|\n",
            "|         4|       N|         N|\n",
            "+----------+--------+----------+\n",
            "\n",
            "+----------+--------+----------+\n",
            "|product_id|low_fats|recyclable|\n",
            "+----------+--------+----------+\n",
            "|         1|       Y|         Y|\n",
            "|         3|       Y|         Y|\n",
            "+----------+--------+----------+\n",
            "\n",
            "+----------+\n",
            "|product_id|\n",
            "+----------+\n",
            "|         1|\n",
            "|         3|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"FilterProducts\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (0, 'Y', 'N'),\n",
        "    (1, 'Y', 'Y'),\n",
        "    (2, 'N', 'Y'),\n",
        "    (3, 'Y', 'Y'),\n",
        "    (4, 'N', 'N')\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"product_id\", \"low_fats\", \"recyclable\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "# Filter where low_fats = 'Y' and recyclable = 'Y'\n",
        "filtered_df = df.filter((col(\"low_fats\") == 'Y') & (col(\"recyclable\") == 'Y'))\n",
        "filtered_df.show()\n",
        "# Select only product_id column\n",
        "result_df = filtered_df.select(\"product_id\")\n",
        "\n",
        "# Show result\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqYRj7mbZ8C2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import length, col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"TweetsFilter\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Let us Code\"),\n",
        "    (2, \"More than fifteen chars are here!\")\n",
        "]\n",
        "columns = [\"tweet_id\", \"content\"]\n",
        "\n",
        "# Create DataFrame\n",
        "tweets_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Filter where content length > 15\n",
        "result_df = tweets_df.filter(length(col(\"content\")) > 15) \\\n",
        "                     .select(\"tweet_id\")\n",
        "\n",
        "result_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import length, col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"TweetsFilter\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Let us Code\"),\n",
        "    (2, \"More than fifteen chars are here!\")\n",
        "]\n",
        "columns = [\"tweet_id\", \"content\"]\n",
        "\n",
        "# Create DataFrame\n",
        "tweets_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "\n",
        "# Register DataFrame as SQL temporary view\n",
        "tweets_df.createOrReplaceTempView(\"Tweets\")\n",
        "\n",
        "# SQL query\n",
        "sql_query = \"\"\"\n",
        "SELECT tweet_id\n",
        "FROM Tweets\n",
        "WHERE LENGTH(content) > 15\n",
        "\"\"\"\n",
        "\n",
        "result_sql_df = spark.sql(sql_query)\n",
        "result_sql_df.show()"
      ],
      "metadata": {
        "id": "dCObEfxtRgJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1121de69-f095-491c-da30-9e3f9d57fd82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|tweet_id|\n",
            "+--------+\n",
            "|       2|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEcgdRUOiDiq8xVFkdRVj8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}